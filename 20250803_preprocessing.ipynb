{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d85617-b19d-4887-8d82-0b0f9eeaf200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ç”Ÿãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿...\n",
      "2. ãƒ©ãƒ™ãƒ«ã®IDåŒ–...\n",
      "ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4722ff06497434cbce87bafe6cbbb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from janome.tokenizer import Tokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ===================================================================\n",
    "# ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©é–¢æ•°ï¼ˆã“ã“ã‹ã‚‰ï¼‰\n",
    "# ===================================================================\n",
    "wakati = Tokenizer()\n",
    "\n",
    "def tokenize_ja(sentences_list):\n",
    "    \"\"\"æ—¥æœ¬èªæ–‡ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\"\"\"\n",
    "    wakati_list = []\n",
    "    print(\"ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "    for sentence in tqdm(sentences_list):\n",
    "        wakati_list.append([item.surface for item in wakati.tokenize(sentence)])\n",
    "    return wakati_list\n",
    "\n",
    "def create_word_id_dict(sentences):\n",
    "    \"\"\"å˜èªã‹ã‚‰IDã¸ã®è¾æ›¸ã‚’ç”Ÿæˆ\"\"\"\n",
    "    word_to_id, id_to_word = {}, {}\n",
    "    # 0ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°/æœªçŸ¥èªç”¨ã«äºˆç´„\n",
    "    word_to_id['<PAD>/<UNK>'] = 0\n",
    "    id_to_word[0] = '<PAD>/<UNK>'\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word_to_id:\n",
    "                tmp_id = len(word_to_id)\n",
    "                word_to_id[word] = tmp_id\n",
    "                id_to_word[tmp_id] = word\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def convert_sentences_to_ids(sentences, word_to_id):\n",
    "    \"\"\"æ–‡ç« ã‚’IDåˆ—ã«å¤‰æ›\"\"\"\n",
    "    sentence_id = []\n",
    "    for sentence in sentences:\n",
    "        sentence_ids = [word_to_id.get(word, 0) for word in sentence] # .getã§é«˜é€ŸåŒ–\n",
    "        sentence_id.append(sentence_ids)\n",
    "    return sentence_id\n",
    "\n",
    "def padding_sentence(sentences):\n",
    "    \"\"\"æ–‡ç« ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†\"\"\"\n",
    "    max_len = max(len(s) for s in sentences) if sentences else 0\n",
    "    \n",
    "    padded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        padding = [0] * (max_len - len(sentence))\n",
    "        padded_sentences.append(padding + sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "# ===================================================================\n",
    "# ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©é–¢æ•°ï¼ˆã“ã“ã¾ã§ï¼‰\n",
    "# ===================================================================\n",
    "\n",
    "# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---\n",
    "print(\"1. ç”Ÿãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿...\")\n",
    "df = pd.read_csv('livedoor_news_corpus.csv') # äº‹å‰ã«ä½œæˆã—ãŸCSV\n",
    "\n",
    "# --- ãƒ©ãƒ™ãƒ«ã®IDåŒ– ---\n",
    "print(\"2. ãƒ©ãƒ™ãƒ«ã®IDåŒ–...\")\n",
    "label_to_id = {label: i for i, label in enumerate(df['label'].unique())}\n",
    "id_to_label = {i: label for i, label in enumerate(df['label'].unique())}\n",
    "df['label_id'] = df['label'].map(label_to_id)\n",
    "\n",
    "# --- ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç† ---\n",
    "# 3. ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆåˆ†ã‹ã¡æ›¸ãï¼‰\n",
    "ja_sentences = tokenize_ja(df['text'].tolist())\n",
    "\n",
    "# 4. å˜èªè¾æ›¸ã®ä½œæˆ\n",
    "print(\"4. å˜èªè¾æ›¸ã®ä½œæˆ...\")\n",
    "word_to_id, id_to_word = create_word_id_dict(ja_sentences)\n",
    "\n",
    "# 5. æ–‡ç« ã‚’IDåˆ—ã«å¤‰æ›\n",
    "print(\"5. æ–‡ç« ã‚’IDåˆ—ã«å¤‰æ›...\")\n",
    "sentence_ids = convert_sentences_to_ids(ja_sentences, word_to_id)\n",
    "\n",
    "# 6. ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†\n",
    "print(\"6. ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†...\")\n",
    "padded_ids = padding_sentence(sentence_ids)\n",
    "\n",
    "# --- ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ ---\n",
    "print(\"7. å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜...\")\n",
    "processed_data = {\n",
    "    'padded_ids': padded_ids,\n",
    "    'labels': df['label_id'].tolist(),\n",
    "    'word_to_id': word_to_id,\n",
    "    'id_to_word': id_to_word,\n",
    "    'label_to_id': label_to_id,\n",
    "    'id_to_label': id_to_label,\n",
    "}\n",
    "\n",
    "with open('processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "\n",
    "print(\"\\nğŸ‰ å‰å‡¦ç†ã¨ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
    "print(f\"ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«: processed_data.pkl\")\n",
    "print(f\"èªå½™æ•°: {len(word_to_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5a8af-b66c-4770-9fa4-dcd76f0017c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
