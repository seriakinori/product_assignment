{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d85617-b19d-4887-8d82-0b0f9eeaf200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 生データの読み込み...\n",
      "2. ラベルのID化...\n",
      "トークン化処理を開始します...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4722ff06497434cbce87bafe6cbbb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from janome.tokenizer import Tokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ===================================================================\n",
    "# ユーザー定義関数（ここから）\n",
    "# ===================================================================\n",
    "wakati = Tokenizer()\n",
    "\n",
    "def tokenize_ja(sentences_list):\n",
    "    \"\"\"日本語文のトークン化\"\"\"\n",
    "    wakati_list = []\n",
    "    print(\"トークン化処理を開始します...\")\n",
    "    for sentence in tqdm(sentences_list):\n",
    "        wakati_list.append([item.surface for item in wakati.tokenize(sentence)])\n",
    "    return wakati_list\n",
    "\n",
    "def create_word_id_dict(sentences):\n",
    "    \"\"\"単語からIDへの辞書を生成\"\"\"\n",
    "    word_to_id, id_to_word = {}, {}\n",
    "    # 0はパディング/未知語用に予約\n",
    "    word_to_id['<PAD>/<UNK>'] = 0\n",
    "    id_to_word[0] = '<PAD>/<UNK>'\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word_to_id:\n",
    "                tmp_id = len(word_to_id)\n",
    "                word_to_id[word] = tmp_id\n",
    "                id_to_word[tmp_id] = word\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def convert_sentences_to_ids(sentences, word_to_id):\n",
    "    \"\"\"文章をID列に変換\"\"\"\n",
    "    sentence_id = []\n",
    "    for sentence in sentences:\n",
    "        sentence_ids = [word_to_id.get(word, 0) for word in sentence] # .getで高速化\n",
    "        sentence_id.append(sentence_ids)\n",
    "    return sentence_id\n",
    "\n",
    "def padding_sentence(sentences):\n",
    "    \"\"\"文章のパディング処理\"\"\"\n",
    "    max_len = max(len(s) for s in sentences) if sentences else 0\n",
    "    \n",
    "    padded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        padding = [0] * (max_len - len(sentence))\n",
    "        padded_sentences.append(padding + sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "# ===================================================================\n",
    "# ユーザー定義関数（ここまで）\n",
    "# ===================================================================\n",
    "\n",
    "# --- メイン処理 ---\n",
    "print(\"1. 生データの読み込み...\")\n",
    "df = pd.read_csv('livedoor_news_corpus.csv') # 事前に作成したCSV\n",
    "\n",
    "# --- ラベルのID化 ---\n",
    "print(\"2. ラベルのID化...\")\n",
    "label_to_id = {label: i for i, label in enumerate(df['label'].unique())}\n",
    "id_to_label = {i: label for i, label in enumerate(df['label'].unique())}\n",
    "df['label_id'] = df['label'].map(label_to_id)\n",
    "\n",
    "# --- テキストの前処理 ---\n",
    "# 3. テキストのトークン化（分かち書き）\n",
    "ja_sentences = tokenize_ja(df['text'].tolist())\n",
    "\n",
    "# 4. 単語辞書の作成\n",
    "print(\"4. 単語辞書の作成...\")\n",
    "word_to_id, id_to_word = create_word_id_dict(ja_sentences)\n",
    "\n",
    "# 5. 文章をID列に変換\n",
    "print(\"5. 文章をID列に変換...\")\n",
    "sentence_ids = convert_sentences_to_ids(ja_sentences, word_to_id)\n",
    "\n",
    "# 6. パディング処理\n",
    "print(\"6. パディング処理...\")\n",
    "padded_ids = padding_sentence(sentence_ids)\n",
    "\n",
    "# --- データの保存 ---\n",
    "print(\"7. 処理済みデータの保存...\")\n",
    "processed_data = {\n",
    "    'padded_ids': padded_ids,\n",
    "    'labels': df['label_id'].tolist(),\n",
    "    'word_to_id': word_to_id,\n",
    "    'id_to_word': id_to_word,\n",
    "    'label_to_id': label_to_id,\n",
    "    'id_to_label': id_to_label,\n",
    "}\n",
    "\n",
    "with open('processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "\n",
    "print(\"\\n🎉 前処理とデータの保存が完了しました。\")\n",
    "print(f\"保存ファイル: processed_data.pkl\")\n",
    "print(f\"語彙数: {len(word_to_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5a8af-b66c-4770-9fa4-dcd76f0017c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
