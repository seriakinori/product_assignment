{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba82326-ad49-447b-9205-7d5abe995904",
   "metadata": {},
   "source": [
    "# テーマ設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accbbf19-be37-4632-a64b-812cb5fadc8c",
   "metadata": {},
   "source": [
    "課題番号**004**の**文章をカテゴリー分類するモデルの作成**に取り組みました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba68b2f-c617-4c4c-a207-25cef0030669",
   "metadata": {},
   "source": [
    "データセットとして**ライブドアニュースコーパス**を使用し、単語分散表現・GRUとCNN、Attentionを組み合わせてより良いモデルを作成しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35c232-dfb5-479c-8659-b3b4c6635305",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00565e2-af6f-41bb-85cd-de1f505921eb",
   "metadata": {},
   "source": [
    "本プロダクト開発課題では、ライブドアニュースコーパスを用いた日本語文章分類の精度向上を目的としてモデルの構築を行いました。9つのカテゴリへの多クラス分類タスクに対し、2つの深層学習モデルを構築し、その性能を比較検証しました。\n",
    "\n",
    "まず、ベースラインとして、局所的な特徴抽出に優れたCNN（畳み込みニューラルネットワーク）と、時系列情報を捉えるGRUを組み合わせたモデルを実装しました。\n",
    "\n",
    "次に、改善モデルとして、Transformerの設計思想に基づき、CNNを自己注意機構（Self-Attention）に置き換え、文全体の構造的・文脈的関係性を捉えるモデルを構築しました。両モデルの実装にはPyTorchを用い、テキストの前処理にはJanomeによる形態素解析を適用しました。\n",
    "\n",
    "実験の結果、ベースラインモデルは正解率83.3%を達成しました。一方、残差接続や層正規化を取り入れた改善モデルは、正解率を89.0%まで向上させ、約5.7ポイントの大幅な性能向上を確認しました。特に、ベースラインが苦手としていた広範なテーマのカテゴリ（例: livedoor-homme, peachy）において、F1スコアの顕著な改善を確認しました（それぞれ0.50→0.65, 0.68→0.77）。\n",
    "\n",
    "以上の結果から、局所的な特徴だけでなく文全体の文脈情報を捉える自己注意機構が、本タスクにおいてCNNよりも優れた特徴抽出器として機能し、分類精度向上に有効であるという結論が得られました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee82c1d4-5686-4a05-b5e5-d78878739512",
   "metadata": {},
   "source": [
    "## 環境構築"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a6bed-673b-44ba-a457-eb20ec4f86c4",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "必要なライブラリをインストールし、実行環境のバージョンを統一します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2db292-6d72-4177-b832-62c96fb9c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Google colab環境であるか判定\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # ライブラリのインストール\n",
    "    %pip install --no-warn-conflicts torch==2.1.1 torchvision==0.16.1 nltk==3.8.1 janome==0.5.0 numpy\n",
    "else:\n",
    "    print(\"Not Google Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a1670-fcc4-4287-bebc-baec24f4331b",
   "metadata": {},
   "source": [
    "### ドライブのマウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c24cebe-c37d-4fa4-9537-13f8a183f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google colab環境であるか判定\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # マウントを行う\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "else:\n",
    "    print(\"Not Google Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de82cddb-78af-434f-846e-7babd2621629",
   "metadata": {},
   "source": [
    "### ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac1120b-aba9-4a81-b84c-f4ad3bbf4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import tarfile\n",
    "import time\n",
    "import urllib.request\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "from nltk import tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e325a-704b-485d-94cb-3ec2f1ae4659",
   "metadata": {},
   "source": [
    "# データ収集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d8be6-1889-45cc-bf70-764500c5ff67",
   "metadata": {},
   "source": [
    "## データセットの準備\n",
    "コーディング試験Chapter11-2で使用したLivedoorニュースコーパスをダウンロードして使用します。\n",
    "インターネット上に公開されているデータセットを以下のコードでダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df2dc26-7ad7-4273-a847-6774c26df55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with urllib.request.urlopen(\"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\") as res:\n",
    "    with open(\"ldcc-20140209.tar.gz\", \"wb\") as f:\n",
    "        f.write(res.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39aac8f-ec87-4dfc-86e8-06940dfe2928",
   "metadata": {},
   "source": [
    "ダウンロードしたファイルは圧縮されているので、作業フォルダに展開します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740385e7-eba5-4411-b3e1-6b88dce0a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pathの設定\n",
    "# Google colab環境であるか判定\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # マイドライブ内のデータを読み込むpathに設定\n",
    "    livedoor_path = \"/content/drive/MyDrive/product_assignment/ldcc-20140209.tar.gz\"\n",
    "else:\n",
    "    livedoor_path = \"ldcc-20140209.tar.gz\"\n",
    "\n",
    "DATA_PATH = \"/content/drive/MyDrive/product_assignment/\"\n",
    "tar = tarfile.open(livedoor_path)\n",
    "tar.extractall(DATA_PATH)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e7af6e-5198-4a85-932a-dc23d1ffda37",
   "metadata": {},
   "source": [
    "## データセットの作成\n",
    "カテゴリをラベル、ファイル内の文章をデータとしてそれらが対になったデータをCSV形式にして保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d448a2-2841-473b-866d-eaf89a5c437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm # 進捗バーを表示\n",
    "\n",
    "# 展開したテキスト群が置かれている親ディレクトリ\n",
    "DATA_DIR = '/content/drive/MyDrive/product_assignment/text/'\n",
    "\n",
    "# カテゴリ名（サブディレクトリ）のリストを取得\n",
    "# 不要なファイル（例：LICENSE.txt）は除外\n",
    "categories = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
    "print(\"対象カテゴリ：\", categories)\n",
    "\n",
    "# 最終的にDataFrameにするための、行データ（辞書）を格納するためのリスト\n",
    "all_data = []\n",
    "\n",
    "# tqdmを使って進捗を表示しながらカテゴリごとにループ\n",
    "for category in tqdm(categories, desc=\"カテゴリ処理中\"):\n",
    "    category_path = os.path.join(data_dir, category)\n",
    "\n",
    "    files = os.listdir(category_path)\n",
    "    for file_name in files:\n",
    "        # category内のREADME.mdはスキップ\n",
    "        if file_name.endwith(\".txt\"):\n",
    "            file_path = os.path.join(category_path, file_name)\n",
    "\n",
    "            try: \n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    # 最初の2行はURLとタイムスタンプなので読み飛ばし、3行目以降を本文として取得\n",
    "                    lines = f.readlines()\n",
    "                    text_body = \"\".join(lines[2:]).strip()\n",
    "\n",
    "                    # ラベル（カテゴリ名）とテキスト本文を辞書としてリストに追加\n",
    "                    all_data.append( {'label': category, 'text': text_body} )\n",
    "            except Exception as e:\n",
    "                print(f\"Error readin {file_path}: {e}\")\n",
    "\n",
    "# ループ完了後、リストから一気にDataFrameを作成\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# 作成したDataFrameをCSVとして保存（インデックスは不要なのでindexにはFalseを設定）\n",
    "df.to_csv(data_dir + \"livedoor_news_corpus.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\nCSVファイルの作成が完了しました。\")\n",
    "print(\"データ件数：\", len(df))\n",
    "print(\"Head：\\n\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0ae41-808d-436e-91f2-d3c7776e13af",
   "metadata": {},
   "source": [
    "## 言語データの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875f7a2-1326-433f-b7bd-a59f90214c7f",
   "metadata": {},
   "source": [
    "日本語を形態素解析して単語表層形に分かち書きします。\n",
    "\n",
    "そのうえで単語をIDに変換します。`\"CUDA out of memory\"` の回避のために文章が512文字を超えた場合には切り詰めを行いました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f7665-cd76-46d6-86ab-f8bd1983d6ff",
   "metadata": {},
   "source": [
    "#### 日本語の分かち書きメソッド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d8597-44b1-400b-a30d-8e767eed054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wakati = Tokenizer()\n",
    "\n",
    "\"\"\" 日本語のトークン化 \"\"\"\n",
    "def tokenize_ja(sentences_list):\n",
    "    wakati_list = []\n",
    "    print(\"トークン処理を開始します。\")\n",
    "    for sentence in tqdm(sentences_list):\n",
    "        # tokenizeから返される表層形を分かち書きリストに登録\n",
    "        wakati_list.append([item.surface for item in wakati.tokenize(sentence)])\n",
    "    return wakati_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3f4da-d807-42bf-aef8-165b98851d0a",
   "metadata": {},
   "source": [
    "#### 単語からIDへの辞書を生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ea4ea-ad7f-4dc2-aa17-1369a308af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 単語からIDへの辞書を作成 \"\"\"\n",
    "def create_word_id_dict(sentences):\n",
    "    word_to_id = {}  # 単語からIDへの変換辞書\n",
    "    id_to_word = {}  # IDから単語への逆引き辞書\n",
    "    # 0はパディング／未知語用に予約\n",
    "    word_to_id['<PAD>/<UNK>'] = 0\n",
    "    id_to_word[0] = '<PAD>/<UNK>'\n",
    "\n",
    "    # すべての文章をループ  \n",
    "    for sentence in sentences:\n",
    "        # 文章内の各単語をループ\n",
    "        for word in sentence:\n",
    "            # もし単語がまだ辞書に登録されていなければ、新しいIDを割り振る、\n",
    "            if word not in word_to_id:\n",
    "                # 新しいIDとして、現在の辞書のサイズ（登録済みの単語数）を使用する\n",
    "                tmp_id = len(word_to_id)\n",
    "                word_to_id[word] = tmp_id\n",
    "                id_to_word[tmp_id] = word\n",
    "\n",
    "    # (単語をキー、IDをバリューとする辞書, IDをキー、単語をバリューとする辞書)のタプルを返す\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315fd19-ecae-4ad4-986e-2731cca61155",
   "metadata": {},
   "source": [
    "#### 文章をID列に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022f826-c6e1-46d2-ae42-da934495e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 単語で構成された文章のリストを対応するIDのリストに変換 \"\"\"\n",
    "def convert_sentences_to_ids(sentences, word_to_id):\n",
    "    sentence_id_list = []\n",
    "    for sentence in sentences:\n",
    "        # dict.get(key, default)メソッドによって、未知語でもエラーにならずにデフォルトである<UNK>のIDを返す\n",
    "        sentence_ids = [word_to_id.get(word, 0) for word in sentence]\n",
    "        sentence_id_list.append( sentence_ids )\n",
    "\n",
    "    # IDに変換された文章のリストを返す \n",
    "    return sentence_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9acd2c7-ceea-4327-ad64-34d7ad908806",
   "metadata": {},
   "source": [
    "#### 文章のパディング処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8cb60-622c-4f81-9541-ca5a86bda9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" IDに変換され文章のリスト\"に対して、paddingと打ち切り処理を行う \"\"\"\n",
    "def padding_and_truncate_sentence(sentences, max_len=512):\n",
    "    # 処理が行われた後の文章IDを格納するリスト\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # １．打ち切り\n",
    "        # 文章の長さが max_lenを超える場合、末尾から max_len分だけを取得します。\n",
    "        # 文章の末尾に重重要な情報含まれる場合が多いため、前から切り捨てます\n",
    "        # 改善モデルにおけるMultiheadAttention層が内部で行う計算で、\n",
    "        # 非常に巨大な行列を作成しようとしてGPUのメモリが足りなくなる問題への対応として実施しました。\n",
    "        sentence = sentence[-max_len:]\n",
    "\n",
    "        # ２．padding\n",
    "        # 文章の長さがmax_lenに満たない場合、差分を計算\n",
    "        padding_size = max_len - len(sentence)\n",
    "\n",
    "        # 足りない分だけ <PAD>のIDのリストを作成し、文章の前方に連結\n",
    "        padding = [0] * padding_size\n",
    "        processed_sentences.append(padding + sentence)\n",
    "\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a483b-33c7-44e5-8ec7-04304b009155",
   "metadata": {},
   "source": [
    "#### 前処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1fbcda-1a31-4959-8c39-bb183cca447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生のCSVの読み込み\n",
    "df = pd.read_csv(DATA_PATH + \"livedoor_news_corpus.csv\")\n",
    "\n",
    "# 文章カテゴリ（ラベル）をID化\n",
    "label_to_id = {label: i for i, label in enumerate(df['label'].unique())}\n",
    "id_to_label = {i: label for i, label in enumerate(df['label'].unique())}\n",
    "\n",
    "# テキストの分かち書き\n",
    "ja_sentences = tokenize_ja(df[\"text\"].tolist())\n",
    "\n",
    "# 単語辞書の作成\n",
    "word_to_id, id_to_word = create_word_id_dict(ja_sentences)\n",
    "\n",
    "# 文章をID列に変換\n",
    "sentence_ids = convert_sentences_to_ids(ja_sentences, word_to_id)\n",
    "\n",
    "# padding処理\n",
    "padded_ids = padding_and_truncate_sentence(sentence_ids)\n",
    "\n",
    "# データをまとめた辞書を作成\n",
    "processed_data = {\n",
    "    'padded_ids': padded_ids,\n",
    "    'labels': df['label_id'].tolist(),\n",
    "    'word_to_id': word_to_id,\n",
    "    'id_to_word': id_to_word,\n",
    "    'label_to_id': label_to_id,\n",
    "    'id_to_label': id_to_label,\n",
    "}\n",
    "\n",
    "# 学習用データはpickle形式にして保存する\n",
    "with open('processed_data_maxlen512.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "\n",
    "print(f\"保存ファイル: processed_data_maxlen512.pkl\")\n",
    "print(f\"語彙数: {len(word_to_id)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14948bf-a9af-4826-8d1a-c7365683e60d",
   "metadata": {},
   "source": [
    "# アルゴリズム選択（ベースラインモデル）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530be25b-0134-4803-8d29-2edceda1879a",
   "metadata": {},
   "source": [
    "## ベースラインモデル設計"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb0faa-8b10-4f7d-a489-d931a825b234",
   "metadata": {},
   "source": [
    "最初にCNNとGRUを1つのディープラーニングモデルの中に層（レイヤー）として組み込み、それぞれの長所を活かす**ハイブリッド**な構造を採用しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd55dae-1fd6-4d64-9be1-223bc3d2cecf",
   "metadata": {},
   "source": [
    "今回はLSTMではなく**GRU**を採用しました。\n",
    "\n",
    "GRUはLSTMと比べてゲートの数が少なく構造がシンプルなため、**計算コストが低く学習が速い**傾向にあります。\n",
    "\n",
    "それでいて、多くのタスクで**LSTMと同等**の性能を発揮することが知られています。\n",
    "\n",
    "今回の課題では、**計算効率**と**実装の容易さ**を考慮し、RNN系手法としてGRUを採用しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1dff5-81a8-4512-a68b-cfd3eabaaa07",
   "metadata": {},
   "source": [
    "CNNは入力された単語ベクトルの並びに対して、**局所的な特徴（n-gramのような短い単語の組み合わせ）**を抽出する役割を果たします。\n",
    "\n",
    "例えば、「とても面白い」や「つまらない」といったキーフレーズを効率的に見つけ出す役割を担います。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e059d5f-c942-4669-af85-d8723e54b180",
   "metadata": {},
   "source": [
    "CNNとGRUの組み合わせを選んだ理由としてこれらの手法の以下の特徴に着目しました：\n",
    "\n",
    "* CNNの長所: 文中の重要なキーワードやフレーズ（局所的な特徴）を効率的に捉えることができる\n",
    "\n",
    "* GRUの長所: RNN系列の手法として、単語の系列（シーケンス）の文脈や順序関係を効果的に捉えることができる\n",
    "\n",
    "この2つを組み合わせることで、**「文章中の重要な部分（CNNが担当）が、どのような文脈で登場したか（GRUが担当）」**を同時に学習できるモデルを作ることができると考えました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e23a37-4ca6-4acc-87be-fc30386ebf50",
   "metadata": {},
   "source": [
    "構築したニューラルネットワークは以下のような**4つの中間層**からなる構成となっています：\n",
    "\n",
    "1. **Embedding層**: 単語をベクトルに変換する層\n",
    "2. **CNN層** (1次元CNN): 特徴を抽出する層\n",
    "3. **GRU層**: 系列情報を処理する再帰的な層\n",
    "4. **全結合層**: 最終的な分類を行う層"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd43084-4595-48ca-be86-857b5df1a4f7",
   "metadata": {},
   "source": [
    "## ベースラインモデル作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8318d903-3ec5-4a29-8046-63d34b900720",
   "metadata": {},
   "source": [
    "### ベースラインモデル (CNN + GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85027378-8fcd-437a-9e22-33b46823480b",
   "metadata": {},
   "source": [
    "### ベースラインモデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fcf5ae-bf5e-4a90-83cf-9c8b0678ddc3",
   "metadata": {},
   "source": [
    "### ベースラインモデルの評価 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76ba50-9667-4743-b3a6-9886a2b42462",
   "metadata": {},
   "source": [
    "### ベースラインモデルの考察"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff1332-889f-4ab0-9251-493304fd5b35",
   "metadata": {},
   "source": [
    "# アルゴリズム選択（改善モデル）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52470d96-8193-460c-9cc8-9cfbf6b6ee25",
   "metadata": {},
   "source": [
    "## 改善モデル設計"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d24e744-e586-4ee4-bbfc-fa20de362969",
   "metadata": {},
   "source": [
    "## 改善モデル作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5289b7-e3c4-4074-80a5-d564a65c4ac0",
   "metadata": {},
   "source": [
    "改善したニューラルネットワークは以下のような**4つの中間層**からなる構成となっています：\n",
    "\n",
    "1. **Embedding層**: 単語をベクトルに変換する層\n",
    "2. **Attentionメカニズム** (Self-Attention): 特徴を抽出する層\n",
    "3. **GRU層**: 系列情報を処理する再帰的な層\n",
    "4. **全結合層**: 最終的な分類を行う層"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02516952-626a-41ea-aded-f7efacd5792f",
   "metadata": {},
   "source": [
    "### 改善モデル（Self-Attention + GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6d8479-3ae2-4a07-acce-1f22f9a6437b",
   "metadata": {},
   "source": [
    "### 改善モデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dcdee2-9e48-4378-abf8-1e006c4ff0d0",
   "metadata": {},
   "source": [
    "### 改善モデルの評価"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f780bb-b205-4713-a6e6-5ef9acf35940",
   "metadata": {},
   "source": [
    "### 改善モデルの考察"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be05a525-44d9-4ee1-b54e-17cbfc9353ac",
   "metadata": {},
   "source": [
    "# 考察と今後の課題"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f77c2d-dea5-4493-bbc0-225b2b55cd07",
   "metadata": {},
   "source": [
    "## 考察"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8242a810-daa5-46cc-8857-36c5eb892960",
   "metadata": {},
   "source": [
    "## 今後の課題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aac192-dec5-4257-812d-f2f1890b5f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
