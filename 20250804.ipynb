{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d82f41-a4ca-4086-b024-f7788e2c4c4c",
   "metadata": {},
   "source": [
    "### ベースラインモデル (CNN + GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe264d97-c255-497c-81b3-8628ae721d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN_GRU_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(CNN_GRU_Model, self).__init__()\n",
    "        \n",
    "        # 1. Embedding層\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # 2. CNN層 (1次元CNN)\n",
    "        self.cnn = nn.Conv1d(in_channels=embedding_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 3. GRU層\n",
    "        self.gru = nn.GRU(input_size=hidden_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        \n",
    "        # 4. 全結合層\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x の形状: (batch_size, seq_len)\n",
    "        \n",
    "        x = self.embedding(x)  # -> (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # CNNは (batch, channels, seq_len) の入力を期待するので、次元を入れ替える\n",
    "        x = x.permute(0, 2, 1) # -> (batch_size, embedding_dim, seq_len)\n",
    "        x = self.cnn(x)        # -> (batch_size, hidden_dim, seq_len)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # GRUの入力に合わせて再度次元を入れ替える\n",
    "        x = x.permute(0, 2, 1) # -> (batch_size, seq_len, hidden_dim)\n",
    "        _, h_n = self.gru(x)   # h_n の形状: (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        # 最後の隠れ状態を使って分類\n",
    "        x = h_n.squeeze(0)     # -> (batch_size, hidden_dim)\n",
    "        out = self.fc(x)       # -> (batch_size, num_classes)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4dd9ff-3b22-41c6-9e21-69038c5e575f",
   "metadata": {},
   "source": [
    "### pklデータの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f27320d-7b07-43fc-a515-afdd21aab7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# A. pklファイルを読み込む\n",
    "with open('processed_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "padded_ids = data['padded_ids'] # これはPythonのリスト\n",
    "labels = data['labels']         # これもPythonのリスト\n",
    "\n",
    "# B. Pythonリストの状態でデータを分割\n",
    "# まずは訓練＋検証用とテスト用に分ける\n",
    "train_val_ids, test_ids, train_val_labels, test_labels = train_test_split(\n",
    "    padded_ids, labels, test_size=0.1, random_state=42, stratify=labels\n",
    ")\n",
    "# 次に訓練用と検証用に分ける\n",
    "train_ids, val_ids, train_labels, val_labels = train_test_split(\n",
    "    train_val_ids, train_val_labels, test_size=(1/9), random_state=42, stratify=train_val_labels\n",
    ")\n",
    "\n",
    "# D. Datasetクラスを定義（__getitem__でTensorに変換）\n",
    "class LivedoorTensorDataset(Dataset):\n",
    "    def __init__(self, ids, labels):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # ここでPythonの数値リストからPyTorchのTensorに変換する\n",
    "        input_ids = torch.tensor(self.ids[index], dtype=torch.long)\n",
    "        label_id = torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        return input_ids, label_id\n",
    "\n",
    "# C. 各データセットのインスタンスを作成\n",
    "train_dataset = LivedoorTensorDataset(train_ids, train_labels)\n",
    "val_dataset = LivedoorTensorDataset(val_ids, val_labels)\n",
    "test_dataset = LivedoorTensorDataset(test_ids, test_labels)\n",
    "\n",
    "# E. DataLoaderを作成\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# これでtrain_loaderから取り出されるデータはTensorのバッチになっている\n",
    "# for batch_ids, batch_labels in train_loader:\n",
    "#     print(batch_ids.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e8b8f-80df-40f7-ae07-66b1d9e7e7db",
   "metadata": {},
   "source": [
    "### 学習・検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bced7d2-12b7-4445-8a90-baac1f88bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# <<< 追加点 (GPUの活用)\n",
    "# ----------------------------------------------------\n",
    "# GPUが利用可能か確認し、deviceを定義\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# モデルを定義し、指定したdeviceに転送\n",
    "# model = CNN_GRU_Model(vocab_size, embedding_dim, hidden_dim, num_classes) # 例\n",
    "model.to(device)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# <<< 修正点 (Optimizerの対象を全パラメータに)\n",
    "# ----------------------------------------------------\n",
    "# model.parameters()を渡すことで、モデルの全ての層のパラメータが学習対象になる\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# これまでの最小の検証損失を保存する変数を初期化（無限大で設定）\n",
    "best_valid_loss = np.inf\n",
    "MODEL_SAVE_PATH = 'best_model.pth' # 保存先のファイルパス\n",
    "\n",
    "n_epochs = 7\n",
    "for epoch in range(n_epochs):\n",
    "    # 各エポックの損失と正解数を初期化\n",
    "    loss_train = 0\n",
    "    loss_valid = 0\n",
    "    accuracy_train = 0\n",
    "    accuracy_valid = 0\n",
    "\n",
    "    # --- 訓練モード ---\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        # <<< 修正点 (データをdeviceに転送)\n",
    "        x = batch[\"input_ids\"].to(device)\n",
    "        t = batch[\"label\"].to(device)\n",
    "        \n",
    "        # 勾配をリセット\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 順伝播\n",
    "        output = model(x)\n",
    "        \n",
    "        # 損失計算と逆伝播\n",
    "        loss = criterion(output, t)\n",
    "        loss.backward()\n",
    "        \n",
    "        # パラメータ更新\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 予測ラベルを計算\n",
    "        pred = output.argmax(dim=1)\n",
    "\n",
    "        # 損失と正解数を加算\n",
    "        loss_train += loss.item()\n",
    "        accuracy_train += torch.sum(pred == t.data)\n",
    "\n",
    "    # --- 検証モード ---\n",
    "    model.eval()\n",
    "    # ----------------------------------------------------\n",
    "    # <<< 修正点 (勾配計算を無効化)\n",
    "    # ----------------------------------------------------\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # <<< 修正点 (データをdeviceに転送)\n",
    "            x = batch[\"input_ids\"].to(device)\n",
    "            t = batch[\"label\"].to(device)\n",
    "            \n",
    "            # 順伝播\n",
    "            output = model(x)\n",
    "            \n",
    "            # 損失計算\n",
    "            loss = criterion(output, t)\n",
    "            \n",
    "            # 予測ラベルを計算\n",
    "            pred = output.argmax(dim=1)\n",
    "\n",
    "            # 損失と正解数を加算\n",
    "            loss_valid += loss.item()\n",
    "            accuracy_valid += torch.sum(pred == t.data)\n",
    "    \n",
    "    # --- エポックごとの結果を計算・表示 ---\n",
    "    # 損失はバッチ数の合計、正解率はデータ数の合計で割る\n",
    "    avg_loss_train = loss_train / len(train_loader)\n",
    "    avg_loss_valid = loss_valid / len(val_loader)\n",
    "    avg_acc_train = accuracy_train / len(train_dataset)\n",
    "    avg_acc_valid = accuracy_valid / len(val_dataset)\n",
    "\n",
    "    # f-stringを使うと、より直感的に記述できる\n",
    "    print(\n",
    "        f\"| epoch {epoch+1:2d} | train loss {avg_loss_train:.4f}, acc {avg_acc_train:.4f} \"\n",
    "        f\"| valid loss {avg_loss_valid:.4f}, acc {avg_acc_valid:.4f}\"\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # ----------------------------------------------------\n",
    "    # 検証データの損失がこれまでの最小値を更新したら、モデルを保存\n",
    "    if avg_loss_valid < best_valid_loss:\n",
    "        print(f\"Validation loss improved ({best_valid_loss:.4f} --> {avg_loss_valid:.4f}). Saving model...\")\n",
    "        best_valid_loss = avg_loss_valid # 最小値を更新\n",
    "        \n",
    "        # モデルのパラメータ（重み）のみを保存するのが推奨される方法\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"\\nTraining finished. Best model saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7685c7b-116f-4a36-9654-9860ccdfa184",
   "metadata": {},
   "source": [
    "### 評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579a5fa-242f-4e55-9653-df7eb42a77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# --- 準備 ---\n",
    "# 学習済みモデルをロードし、deviceに転送\n",
    "# model.load_state_dict(torch.load('best_model.pth')) # 例\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 予測結果と正解ラベルを格納するリスト\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# --- 評価モード ---\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader: # test_loaderを使用\n",
    "        # データをdeviceに転送\n",
    "        x = batch[\"input_ids\"].to(device)\n",
    "        t = batch[\"label\"].to(device)\n",
    "        \n",
    "        # モデルの出力を取得\n",
    "        output = model(x)\n",
    "        \n",
    "        # 予測ラベルを計算 (argmaxで最も確率の高いクラスを取得)\n",
    "        pred = output.argmax(dim=1)\n",
    "        \n",
    "        # 予測結果と正解ラベルをリストに追加\n",
    "        # scikit-learnで計算するために、GPU上のTensorからCPU上のnumpy配列に変換\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(t.cpu().numpy())\n",
    "\n",
    "# --- 評価指標の計算 (scikit-learnを使用) ---\n",
    "# 正解率\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# カテゴリ名のリスト（id_to_label辞書から取得）\n",
    "# category_names = list(id_to_label.values()) # 例\n",
    "\n",
    "# 適合率、再現率、F1スコアを含む詳細レポート\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(all_labels, all_preds, target_names=category_names)\n",
    "print(report)\n",
    "\n",
    "# -------------テキストファイルへの保存--------------------\n",
    "with open(\"test_results.txt\", \"w\") as f:\n",
    "f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "f.write(\"\\nClassification Report:\\n\")\n",
    "f.write(report)\n",
    "\n",
    "print(\"\\nTest results saved to test_results.txt\")\n",
    "\n",
    "# --- 混同行列の可視化 ---\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# DataFrameに変換すると、軸ラベルが見やすくなる\n",
    "cm_df = pd.DataFrame(cm, index=category_names, columns=category_names)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout() # ラベルが切れないように調整\n",
    "\n",
    "#-------------混同行列の画像ファイルへの保存------------\n",
    "\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "print(\"\\nConfusion matrix saved to confusion_matrix.png\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
