{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f96a6bed-673b-44ba-a457-eb20ec4f86c4",
   "metadata": {},
   "source": [
    "### ライブラリのインストール\n",
    "必要なライブラリをインストールし、実行環境のバージョンを統一します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2db292-6d72-4177-b832-62c96fb9c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Google colab環境であるか判定\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # ライブラリのインストール\n",
    "    %pip install --no-warn-conflicts torch==2.1.1 torchvision==0.16.1 nltk==3.8.1 janome==0.5.0\n",
    "else:\n",
    "    print(\"Not Google Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a1670-fcc4-4287-bebc-baec24f4331b",
   "metadata": {},
   "source": [
    "### ドライブのマウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c24cebe-c37d-4fa4-9537-13f8a183f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google colab環境であるか判定\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # マウントを行う\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "else:\n",
    "    print(\"Not Google Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de82cddb-78af-434f-846e-7babd2621629",
   "metadata": {},
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac1120b-aba9-4a81-b84c-f4ad3bbf4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import tarfile\n",
    "import time\n",
    "import urllib.request\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#import spacy\n",
    "from janome.tokenizer import Tokenizer\n",
    "from nltk import tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e325a-704b-485d-94cb-3ec2f1ae4659",
   "metadata": {},
   "source": [
    "### データセットの用意\n",
    "コーディング試験11-2で既にダウンロードしていたLivedoorニュースコーパスを使用します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740385e7-eba5-4411-b3e1-6b88dce0a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pathの設定\n",
    "# Google colab環境であるか判定\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # マイドライブ内のデータを読み込むpathに設定\n",
    "    livedoor_path = \"/content/drive/MyDrive/ldcc-20140209.tar.gz\"\n",
    "else:\n",
    "    livedoor_path = \"ldcc-20140209.tar.gz\"\n",
    "\n",
    "save_path = \"./data/livedoor/\"\n",
    "tar = tarfile.open(livedoor_path)\n",
    "tar.extractall(save_path)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e7af6e-5198-4a85-932a-dc23d1ffda37",
   "metadata": {},
   "source": [
    "#### データセットの作成\n",
    "カテゴリをラベル、ファイル内の文章をデータとしてそれらが対になったデータをCSV形式にして保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8cccaa-5b06-4aef-bf91-c4eaf63b000b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# カテゴリ名（サブディレクトリ名）のリストを取得\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 不要なファイル（例: LICENSE.txt）は除外\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m categories \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(data_dir) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, d))]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m対象カテゴリ:\u001b[39m\u001b[38;5;124m\"\u001b[39m, categories)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 最終的にDataFrameにするための、行データ（辞書）を格納するリスト\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm # 進捗バーを表示\n",
    "\n",
    "# ライブドアニュースコーパスのテキストファイル群が置かれている親ディレクトリ\n",
    "# 例: /content/text/\n",
    "data_dir = 'text'\n",
    "\n",
    "# カテゴリ名（サブディレクトリ名）のリストを取得\n",
    "# 不要なファイル（例: LICENSE.txt）は除外\n",
    "categories = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "print(\"対象カテゴリ:\", categories)\n",
    "\n",
    "# 最終的にDataFrameにするための、行データ（辞書）を格納するリスト\n",
    "all_data = []\n",
    "\n",
    "# tqdmを使って進捗を可視化しながらループ\n",
    "for category in tqdm(categories, desc=\"カテゴリ処理中\"):\n",
    "    category_path = os.path.join(data_dir, category)\n",
    "    \n",
    "    files = os.listdir(category_path)\n",
    "    for file_name in files:\n",
    "        # category内のREADME.mdはスキップ\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(category_path, file_name)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    # 最初の2行はURLとタイムスタンプなので読み飛ばし、3行目以降を本文とする\n",
    "                    lines = f.readlines()\n",
    "                    text_body = \"\".join(lines[2:]).strip()\n",
    "                    \n",
    "                    # ラベル（カテゴリ名）とテキスト本文を辞書としてリストに追加\n",
    "                    all_data.append({'label': category, 'text': text_body})\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# ループ完了後、リストから一気にDataFrameを作成\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# 作成したDataFrameをCSVとして保存（インデックスは不要なのでFalse）\n",
    "df.to_csv('livedoor_news_corpus.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\nCSVファイルの作成が完了しました。\")\n",
    "print(\"データ件数:\", len(df))\n",
    "print(\"最初の5件:\\n\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f7665-cd76-46d6-86ab-f8bd1983d6ff",
   "metadata": {},
   "source": [
    "#### 日本語の分かち書きメソッド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77634f5-e79d-44db-b2c3-cfbb0238ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from janome.tokenizer import Tokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ===================================================================\n",
    "# ユーザー定義関数（ここから）\n",
    "# ===================================================================\n",
    "wakati = Tokenizer()\n",
    "\n",
    "def tokenize_ja(sentences_list):\n",
    "    \"\"\"日本語文のトークン化\"\"\"\n",
    "    wakati_list = []\n",
    "    print(\"トークン化処理を開始します...\")\n",
    "    for sentence in tqdm(sentences_list):\n",
    "        wakati_list.append([item.surface for item in wakati.tokenize(sentence)])\n",
    "    return wakati_list\n",
    "\n",
    "def create_word_id_dict(sentences):\n",
    "    \"\"\"単語からIDへの辞書を生成\"\"\"\n",
    "    word_to_id, id_to_word = {}, {}\n",
    "    # 0はパディング/未知語用に予約\n",
    "    word_to_id['<PAD>/<UNK>'] = 0\n",
    "    id_to_word[0] = '<PAD>/<UNK>'\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word_to_id:\n",
    "                tmp_id = len(word_to_id)\n",
    "                word_to_id[word] = tmp_id\n",
    "                id_to_word[tmp_id] = word\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def convert_sentences_to_ids(sentences, word_to_id):\n",
    "    \"\"\"文章をID列に変換\"\"\"\n",
    "    sentence_id = []\n",
    "    for sentence in sentences:\n",
    "        sentence_ids = [word_to_id.get(word, 0) for word in sentence] # .getで高速化\n",
    "        sentence_id.append(sentence_ids)\n",
    "    return sentence_id\n",
    "\n",
    "def padding_sentence(sentences):\n",
    "    \"\"\"文章のパディング処理\"\"\"\n",
    "    max_len = max(len(s) for s in sentences) if sentences else 0\n",
    "    \n",
    "    padded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        padding = [0] * (max_len - len(sentence))\n",
    "        padded_sentences.append(padding + sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "# ===================================================================\n",
    "# ユーザー定義関数（ここまで）\n",
    "# ===================================================================\n",
    "\n",
    "# --- メイン処理 ---\n",
    "print(\"1. 生データの読み込み...\")\n",
    "df = pd.read_csv('livedoor_news_corpus.csv') # 事前に作成したCSV\n",
    "\n",
    "# --- ラベルのID化 ---\n",
    "print(\"2. ラベルのID化...\")\n",
    "label_to_id = {label: i for i, label in enumerate(df['label'].unique())}\n",
    "id_to_label = {i: label for i, label in enumerate(df['label'].unique())}\n",
    "df['label_id'] = df['label'].map(label_to_id)\n",
    "\n",
    "# --- テキストの前処理 ---\n",
    "# 3. テキストのトークン化（分かち書き）\n",
    "ja_sentences = tokenize_ja(df['text'].tolist())\n",
    "\n",
    "# 4. 単語辞書の作成\n",
    "print(\"4. 単語辞書の作成...\")\n",
    "word_to_id, id_to_word = create_word_id_dict(ja_sentences)\n",
    "\n",
    "# 5. 文章をID列に変換\n",
    "print(\"5. 文章をID列に変換...\")\n",
    "sentence_ids = convert_sentences_to_ids(ja_sentences, word_to_id)\n",
    "\n",
    "# 6. パディング処理\n",
    "print(\"6. パディング処理...\")\n",
    "padded_ids = padding_sentence(sentence_ids)\n",
    "\n",
    "# --- データの保存 ---\n",
    "print(\"7. 処理済みデータの保存...\")\n",
    "processed_data = {\n",
    "    'padded_ids': padded_ids,\n",
    "    'labels': df['label_id'].tolist(),\n",
    "    'word_to_id': word_to_id,\n",
    "    'id_to_word': id_to_word,\n",
    "    'label_to_id': label_to_id,\n",
    "    'id_to_label': id_to_label,\n",
    "}\n",
    "\n",
    "with open('processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "\n",
    "print(\"\\n🎉 前処理とデータの保存が完了しました。\")\n",
    "print(f\"保存ファイル: processed_data.pkl\")\n",
    "print(f\"語彙数: {len(word_to_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd135eaa-d60c-4bae-be8f-db541807a381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00905d48-601b-4f49-9986-0272bb771e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "wakati = Tokenizer()\n",
    "\n",
    "\n",
    "def tokenize_ja(sentences_list):\n",
    "    \"\"\"日本語文のトークン化\n",
    "    与えられた日本語の文のリストをトークン化します。各文は単語に分割され、リストとして返されます。\n",
    "\n",
    "    Args:\n",
    "        sentences_list (list): トークン化する日本語の文のリスト。\n",
    "\n",
    "    Returns:\n",
    "        list: トークン化された文のリスト。各要素は単語のリスト。\n",
    "\n",
    "    Examples:\n",
    "        >>> sentences = [\"こんにちは世界\", \"これはテスト文です。\"]\n",
    "        >>> tokenize_ja(sentences)\n",
    "        [['こんにちは', '世界'], ['これ', 'は', 'テスト', '文', 'です', '。']]\n",
    "\n",
    "    Note:\n",
    "        - この関数はjanomeライブラリの `Tokenizer` クラスを使用しています。\n",
    "        - 与えられた各文に対してトークン化処理を行い、結果をリストで返します。\n",
    "        - 日本語のトークン化では、形態素解析を行い、各形態素の表層形を抽出します。\n",
    "    \"\"\"\n",
    "    wakati_list = []\n",
    "\n",
    "    for sentence in sentences_list:\n",
    "        wakati_list.append([item.surface for item in wakati.tokenize(sentence)])\n",
    "    return wakati_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a94d1a7-bf61-44b1-a204-f05f6b35952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_sentences = tokenize_ja([\"与えられた日本語の文のリストをトークン化します。\",\"各文は単語に分割され、リストとして返されます。\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b83731b4-4389-4de4-b385-2d91e307f31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['与え', 'られ', 'た', '日本語', 'の', '文', 'の', 'リスト', 'を', 'トー', 'クン', '化', 'し', 'ます', '。'], ['各', '文', 'は', '単語', 'に', '分割', 'さ', 'れ', '、', 'リスト', 'として', '返さ', 'れ', 'ます', '。']]\n"
     ]
    }
   ],
   "source": [
    "print(ja_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3f4da-d807-42bf-aef8-165b98851d0a",
   "metadata": {},
   "source": [
    "#### 単語からIDへの辞書を生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d3d5f2b-5c00-4976-a3c8-d9f7d897de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_id_dict(sentences):\n",
    "    \"\"\"単語からIDへの辞書を生成\n",
    "    与えられた文のリストから、単語をIDに変換するための辞書を生成します。\n",
    "\n",
    "    Args:\n",
    "        sentences (list): 単語のリストを含む文のリスト。\n",
    "\n",
    "    Returns:\n",
    "        dict: 単語からIDへのマッピングを含む辞書。(word_to_id)\n",
    "                 IDから単語へのマッピングを含む辞書。(id_to_word)\n",
    "\n",
    "    Note:\n",
    "        - 各単語に一意のIDを割り当てます。\n",
    "        - 未登録の単語があれば新しいIDを割り当てます。\n",
    "    \"\"\"\n",
    "    word_to_id, id_to_word = {}, {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word_to_id:\n",
    "                tmp_id = len(word_to_id) + 1\n",
    "                word_to_id[word] = tmp_id\n",
    "                id_to_word[tmp_id] = word\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315fd19-ecae-4ad4-986e-2731cca61155",
   "metadata": {},
   "source": [
    "#### 文章をID列に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ce92b66-fbb5-46b8-99ac-af1d5da150cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentences_to_ids(sentences, word_to_id):\n",
    "    \"\"\"文章をID列に変換\n",
    "    与えられた文を、単語IDのリストに変換します。\n",
    "\n",
    "    Args:\n",
    "        sentences (list): 単語のリストを含む文のリスト。\n",
    "        word_to_id (dict): 単語からIDへのマッピングを含む辞書。\n",
    "\n",
    "    Returns:\n",
    "        list: 単語IDのリストを含む文のリスト。\n",
    "\n",
    "    Note:\n",
    "        - 辞書に登録されていない単語はID 0にマッピングします。\n",
    "    \"\"\"\n",
    "    sentence_id = []\n",
    "    for sentence in sentences:\n",
    "        sentence_ids = []\n",
    "        for word in sentence:\n",
    "            if word in word_to_id:\n",
    "                sentence_ids.append(word_to_id[word])\n",
    "            else:\n",
    "                sentence_ids.append(0)\n",
    "        sentence_id.append(sentence_ids)\n",
    "    return sentence_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9acd2c7-ceea-4327-ad64-34d7ad908806",
   "metadata": {},
   "source": [
    "#### 文章のパディング処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c6e59c5-ca61-4817-bc6b-f853281c6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_sentence(sentences):\n",
    "    \"\"\"文章のパディング処理\n",
    "    与えられた文のリストにパディングを施し、全ての文の長さを統一します。\n",
    "\n",
    "    Args:\n",
    "        sentences (list): 単語IDのリストを含む文のリスト。\n",
    "\n",
    "    Returns:\n",
    "        list: パディングされた文のリスト。\n",
    "\n",
    "    Note:\n",
    "        - 最も長い文の長さに合わせて他の文をパディングします。\n",
    "        - パディングにはID 0を使用します。\n",
    "    \"\"\"\n",
    "    max_sentence_size = 0\n",
    "    for sentence in sentences:\n",
    "        if max_sentence_size < len(sentence):\n",
    "            max_sentence_size = len(sentence)\n",
    "    for sentence in sentences:\n",
    "        while len(sentence) < max_sentence_size:\n",
    "            sentence.insert(0, 0)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51e804fd-333e-45b3-bd85-add7fc3f0661",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m ja_sentences \u001b[38;5;241m=\u001b[39m raw_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# test for 0:99\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m ja_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_ja\u001b[49m\u001b[43m(\u001b[49m\u001b[43mja_sentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m ja_word_to_id, ja_id_to_word \u001b[38;5;241m=\u001b[39m create_word_id_dict(ja_sentences)\n\u001b[0;32m      6\u001b[0m ja_sentences \u001b[38;5;241m=\u001b[39m convert_sentences_to_ids(ja_sentences, ja_word_to_id)\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36mtokenize_ja\u001b[1;34m(sentences_list)\u001b[0m\n\u001b[0;32m     24\u001b[0m wakati_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences_list:\n\u001b[1;32m---> 27\u001b[0m     wakati_list\u001b[38;5;241m.\u001b[39mappend([item\u001b[38;5;241m.\u001b[39msurface \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m wakati\u001b[38;5;241m.\u001b[39mtokenize(sentence)])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wakati_list\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     24\u001b[0m wakati_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences_list:\n\u001b[1;32m---> 27\u001b[0m     wakati_list\u001b[38;5;241m.\u001b[39mappend([item\u001b[38;5;241m.\u001b[39msurface \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m wakati\u001b[38;5;241m.\u001b[39mtokenize(sentence)])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wakati_list\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\tokenizer.py:226\u001b[0m, in \u001b[0;36mTokenizer.__tokenize_stream\u001b[1;34m(self, text, wakati, baseform_unk, dotfile)\u001b[0m\n\u001b[0;32m    224\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m processed \u001b[38;5;241m<\u001b[39m text_length:\n\u001b[1;32m--> 226\u001b[0m     tokens, pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__tokenize_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwakati\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseform_unk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdotfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m token\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\tokenizer.py:250\u001b[0m, in \u001b[0;36mTokenizer.__tokenize_partial\u001b[1;34m(self, text, wakati, baseform_unk, dotfile)\u001b[0m\n\u001b[0;32m    248\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msys_dic\u001b[38;5;241m.\u001b[39mlookup(encoded_partial_text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatcher)\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m entries:\n\u001b[1;32m--> 250\u001b[0m     \u001b[43mlattice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSurfaceNode\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNodeType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSYS_DICT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m matched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(entries) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# unknown\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\lattice.py:139\u001b[0m, in \u001b[0;36mLattice.add\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    137\u001b[0m dic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdic\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m enode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menodes[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp]:\n\u001b[1;32m--> 139\u001b[0m     cost \u001b[38;5;241m=\u001b[39m enode\u001b[38;5;241m.\u001b[39mmin_cost \u001b[38;5;241m+\u001b[39m \u001b[43mdic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trans_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43menode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_left_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cost \u001b[38;5;241m<\u001b[39m min_cost:\n\u001b[0;32m    141\u001b[0m         min_cost, best_node \u001b[38;5;241m=\u001b[39m cost, enode\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\dic.py:319\u001b[0m, in \u001b[0;36mMMapDictionary.get_trans_cost\u001b[1;34m(self, id1, id2)\u001b[0m\n\u001b[0;32m    316\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mformat_exc()\n\u001b[0;32m    317\u001b[0m         sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_trans_cost\u001b[39m(\u001b[38;5;28mself\u001b[39m, id1, id2):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnections[id1][id2]\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__del__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test for 0:99\n",
    "raw_df = pd.read_csv('livedoor_news_corpus.csv')[0:99]\n",
    "ja_sentences = raw_df[\"text\"].values\n",
    "ja_sentences = tokenize_ja(ja_sentences)\n",
    "ja_word_to_id, ja_id_to_word = create_word_id_dict(ja_sentences)\n",
    "ja_sentences = convert_sentences_to_ids(ja_sentences, ja_word_to_id)\n",
    "ja_sentences = padding_sentence(ja_sentences)\n",
    "ja_sentences = np.array(ja_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8cb60-622c-4f81-9541-ca5a86bda9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_sentences[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fbeead-2aeb-47b1-a94b-7756f9fefa7c",
   "metadata": {},
   "source": [
    "#### 層化サンプリング（Stratified Sampling）\n",
    "データが少ないカテゴリも、訓練・検証・テストの各データセットに均等に分配されるため、モデルの性能を偏りなく正確に評価できます。\n",
    "\n",
    "もし単純なランダムサンプリングを行うと、偶然、特定のカテゴリのデータがテストデータにほとんど含まれない、といった偏りが生じる可能性があります。\n",
    "stratify=yを指定するだけで、そうした事故を防ぎ、信頼性の高いモデル評価ができるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "500c84a8-82fe-4e87-b35c-0b5d8d773245",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m ja_sentences\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 特徴量X（文章）とラベルy（カテゴリ）を定義\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 1. 第1段階：訓練＋検証データ(90%)とテストデータ(10%)に分割\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# stratify=y を指定することで、yの比率を保ったまま分割される\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 前処理済みのCSVファイルを読み込む\n",
    "df = ja_sentences\n",
    "\n",
    "#df = ja_sentences\n",
    "# 特徴量X（文章）とラベルy（カテゴリ）を定義\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. 第1段階：訓練＋検証データ(90%)とテストデータ(10%)に分割\n",
    "# stratify=y を指定することで、yの比率を保ったまま分割される\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.1,      # 全体の10%をテストデータに\n",
    "    random_state=42,    # 再現性のための乱数シード\n",
    "    stratify=y          # 層化サンプリングを有効化\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. 第2段階：訓練＋検証データを、訓練データと検証データに分割\n",
    "# 元の90%から、さらに1/9を検証データにすると、全体比率が80%:10%になる\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, \n",
    "    test_size=(1/9),    # 90%のうちの1/9、つまり全体の10%\n",
    "    random_state=42,\n",
    "    stratify=y_train_val # こちらも層化\n",
    ")\n",
    "# --------------------------------------------------\n",
    "\n",
    "# 各データセットのサイズを確認\n",
    "print(f\"訓練データ   : {len(X_train)}件\")\n",
    "print(f\"検証データ   : {len(X_val)}件\")\n",
    "print(f\"テストデータ : {len(X_test)}件\")\n",
    "\n",
    "print(\"\\n--- 各セットのカテゴリ比率 ---\")\n",
    "print(\"元データ:\\n\", y.value_counts(normalize=True).sort_index())\n",
    "print(\"\\n訓練データ:\\n\", y_train.value_counts(normalize=True).sort_index())\n",
    "print(\"\\nテストデータ:\\n\", y_test.value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9424bf71-f684-406c-827a-99131ee4f005",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# GiNZAなどのTokenizerは準備済みと仮定\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 1. 生データを読み込み、分かち書き\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlivedoor_news_corpus.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_ja\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 分かち書き関数\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 2. 語彙（word_to_id辞書）を構築\u001b[39;00m\n\u001b[0;32m     12\u001b[0m word_counter \u001b[38;5;241m=\u001b[39m Counter(word \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36mtokenize_ja\u001b[1;34m(sentences_list)\u001b[0m\n\u001b[0;32m     24\u001b[0m wakati_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences_list:\n\u001b[1;32m---> 27\u001b[0m     wakati_list\u001b[38;5;241m.\u001b[39mappend([item\u001b[38;5;241m.\u001b[39msurface \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m wakati\u001b[38;5;241m.\u001b[39mtokenize(sentence)])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wakati_list\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     24\u001b[0m wakati_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences_list:\n\u001b[1;32m---> 27\u001b[0m     wakati_list\u001b[38;5;241m.\u001b[39mappend([item\u001b[38;5;241m.\u001b[39msurface \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m wakati\u001b[38;5;241m.\u001b[39mtokenize(sentence)])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wakati_list\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\tokenizer.py:226\u001b[0m, in \u001b[0;36mTokenizer.__tokenize_stream\u001b[1;34m(self, text, wakati, baseform_unk, dotfile)\u001b[0m\n\u001b[0;32m    224\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m processed \u001b[38;5;241m<\u001b[39m text_length:\n\u001b[1;32m--> 226\u001b[0m     tokens, pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__tokenize_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwakati\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseform_unk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdotfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m token\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\tokenizer.py:248\u001b[0m, in \u001b[0;36mTokenizer.__tokenize_partial\u001b[1;34m(self, text, wakati, baseform_unk, dotfile)\u001b[0m\n\u001b[0;32m    245\u001b[0m     matched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(entries) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# system dictionary\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msys_dic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_partial_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m entries:\n\u001b[0;32m    250\u001b[0m     lattice\u001b[38;5;241m.\u001b[39madd(SurfaceNode(e, NodeType\u001b[38;5;241m.\u001b[39mSYS_DICT))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\dic.py:253\u001b[0m, in \u001b[0;36mMMapDictionary.lookup\u001b[1;34m(self, s, matcher)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlookup\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, matcher):\n\u001b[1;32m--> 253\u001b[0m     (matched, outputs) \u001b[38;5;241m=\u001b[39m \u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matched:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\fst.py:341\u001b[0m, in \u001b[0;36mMatcher.run\u001b[1;34m(self, word, common_prefix_match)\u001b[0m\n\u001b[0;32m    339\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict_len):\n\u001b[1;32m--> 341\u001b[0m     output \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_prefix_match\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(output), output\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\fst.py:364\u001b[0m, in \u001b[0;36mMatcher._run\u001b[1;34m(self, word, data_num, common_prefix_match)\u001b[0m\n\u001b[0;32m    361\u001b[0m         i \u001b[38;5;241m=\u001b[39m j\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_len\u001b[49m:\n\u001b[0;32m    365\u001b[0m     arc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_arc(data, pos)\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;66;03m# arc[0]: flag\u001b[39;00m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;66;03m# arc[1]: label\u001b[39;00m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;66;03m# arc[2]: output\u001b[39;00m\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;66;03m# arc[3]: final_output\u001b[39;00m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;66;03m# arc[4]: target\u001b[39;00m\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;66;03m# arc[5]: incr\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "# GiNZAなどのTokenizerは準備済みと仮定\n",
    "\n",
    "\n",
    "# 1. 生データを読み込み、分かち書き\n",
    "df = pd.read_csv('livedoor_news_corpus.csv')\n",
    "df['tokens'] = df['text'].apply(tokenize_ja) # 分かち書き関数\n",
    "\n",
    "# 2. 語彙（word_to_id辞書）を構築\n",
    "word_counter = Counter(word for tokens in df['tokens'] for word in tokens)\n",
    "word_to_id = {word: i+2 for i, (word, count) in enumerate(word_counter.items())} # 0と1は特殊トークン用\n",
    "word_to_id['<PAD>'] = 0\n",
    "word_to_id['<UNK>'] = 1\n",
    "\n",
    "# 3. テキストとラベルをIDのシーケンスに変換\n",
    "df['input_ids'] = df['tokens'].apply(lambda tokens: [word_to_id.get(w, word_to_id['<UNK>']) for w in tokens])\n",
    "# ラベルのID化...\n",
    "\n",
    "# 4. 必要なデータをファイルに保存\n",
    "processed_data = {\n",
    "    'input_ids': df['input_ids'].tolist(),\n",
    "    'labels': df['label_id'].tolist(),\n",
    "    'word_to_id': word_to_id,\n",
    "    # 他にも必要なデータがあれば...\n",
    "}\n",
    "\n",
    "with open('processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "\n",
    "print(\"前処理とデータの保存が完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d84475-4430-45b6-b792-35bf34505d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
