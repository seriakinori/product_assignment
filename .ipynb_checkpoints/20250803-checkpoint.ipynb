{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f96a6bed-673b-44ba-a457-eb20ec4f86c4",
   "metadata": {},
   "source": [
    "### ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€å®Ÿè¡Œç’°å¢ƒã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’çµ±ä¸€ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2db292-6d72-4177-b832-62c96fb9c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Google colabç’°å¢ƒã§ã‚ã‚‹ã‹åˆ¤å®š\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    %pip install --no-warn-conflicts torch==2.1.1 torchvision==0.16.1 nltk==3.8.1 janome==0.5.0\n",
    "else:\n",
    "    print(\"Not Google Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a1670-fcc4-4287-bebc-baec24f4331b",
   "metadata": {},
   "source": [
    "### ãƒ‰ãƒ©ã‚¤ãƒ–ã®ãƒã‚¦ãƒ³ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c24cebe-c37d-4fa4-9537-13f8a183f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google colabç’°å¢ƒã§ã‚ã‚‹ã‹åˆ¤å®š\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # ãƒã‚¦ãƒ³ãƒˆã‚’è¡Œã†\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "else:\n",
    "    print(\"Not Google Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de82cddb-78af-434f-846e-7babd2621629",
   "metadata": {},
   "source": [
    "## ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac1120b-aba9-4a81-b84c-f4ad3bbf4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import tarfile\n",
    "import time\n",
    "import urllib.request\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#import spacy\n",
    "from janome.tokenizer import Tokenizer\n",
    "from nltk import tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e325a-704b-485d-94cb-3ec2f1ae4659",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”¨æ„\n",
    "ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è©¦é¨“11-2ã§æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ãŸLivedoorãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740385e7-eba5-4411-b3e1-6b88dce0a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pathã®è¨­å®š\n",
    "# Google colabç’°å¢ƒã§ã‚ã‚‹ã‹åˆ¤å®š\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€pathã«è¨­å®š\n",
    "    livedoor_path = \"/content/drive/MyDrive/ldcc-20140209.tar.gz\"\n",
    "else:\n",
    "    livedoor_path = \"ldcc-20140209.tar.gz\"\n",
    "\n",
    "save_path = \"./data/livedoor/\"\n",
    "tar = tarfile.open(livedoor_path)\n",
    "tar.extractall(save_path)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e7af6e-5198-4a85-932a-dc23d1ffda37",
   "metadata": {},
   "source": [
    "#### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ\n",
    "ã‚«ãƒ†ã‚´ãƒªã‚’ãƒ©ãƒ™ãƒ«ã€ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®æ–‡ç« ã‚’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ãã‚Œã‚‰ãŒå¯¾ã«ãªã£ãŸãƒ‡ãƒ¼ã‚¿ã‚’CSVå½¢å¼ã«ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b8cccaa-5b06-4aef-bf91-c4eaf63b000b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# ã‚«ãƒ†ã‚´ãƒªåï¼ˆã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåï¼‰ã®ãƒªã‚¹ãƒˆã‚’å–å¾—\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# ä¸è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¾‹: LICENSE.txtï¼‰ã¯é™¤å¤–\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m categories \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(data_dir) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, d))]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124må¯¾è±¡ã‚«ãƒ†ã‚´ãƒª:\u001b[39m\u001b[38;5;124m\"\u001b[39m, categories)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# æœ€çµ‚çš„ã«DataFrameã«ã™ã‚‹ãŸã‚ã®ã€è¡Œãƒ‡ãƒ¼ã‚¿ï¼ˆè¾æ›¸ï¼‰ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm # é€²æ—ãƒãƒ¼ã‚’è¡¨ç¤º\n",
    "\n",
    "# ãƒ©ã‚¤ãƒ–ãƒ‰ã‚¢ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ãŒç½®ã‹ã‚Œã¦ã„ã‚‹è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "# ä¾‹: /content/text/\n",
    "data_dir = 'text'\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªåï¼ˆã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåï¼‰ã®ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "# ä¸è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¾‹: LICENSE.txtï¼‰ã¯é™¤å¤–\n",
    "categories = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "print(\"å¯¾è±¡ã‚«ãƒ†ã‚´ãƒª:\", categories)\n",
    "\n",
    "# æœ€çµ‚çš„ã«DataFrameã«ã™ã‚‹ãŸã‚ã®ã€è¡Œãƒ‡ãƒ¼ã‚¿ï¼ˆè¾æ›¸ï¼‰ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ\n",
    "all_data = []\n",
    "\n",
    "# tqdmã‚’ä½¿ã£ã¦é€²æ—ã‚’å¯è¦–åŒ–ã—ãªãŒã‚‰ãƒ«ãƒ¼ãƒ—\n",
    "for category in tqdm(categories, desc=\"ã‚«ãƒ†ã‚´ãƒªå‡¦ç†ä¸­\"):\n",
    "    category_path = os.path.join(data_dir, category)\n",
    "    \n",
    "    files = os.listdir(category_path)\n",
    "    for file_name in files:\n",
    "        # categoryå†…ã®README.mdã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(category_path, file_name)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    # æœ€åˆã®2è¡Œã¯URLã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãªã®ã§èª­ã¿é£›ã°ã—ã€3è¡Œç›®ä»¥é™ã‚’æœ¬æ–‡ã¨ã™ã‚‹\n",
    "                    lines = f.readlines()\n",
    "                    text_body = \"\".join(lines[2:]).strip()\n",
    "                    \n",
    "                    # ãƒ©ãƒ™ãƒ«ï¼ˆã‚«ãƒ†ã‚´ãƒªåï¼‰ã¨ãƒ†ã‚­ã‚¹ãƒˆæœ¬æ–‡ã‚’è¾æ›¸ã¨ã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "                    all_data.append({'label': category, 'text': text_body})\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# ãƒ«ãƒ¼ãƒ—å®Œäº†å¾Œã€ãƒªã‚¹ãƒˆã‹ã‚‰ä¸€æ°—ã«DataFrameã‚’ä½œæˆ\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# ä½œæˆã—ãŸDataFrameã‚’CSVã¨ã—ã¦ä¿å­˜ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ä¸è¦ãªã®ã§Falseï¼‰\n",
    "df.to_csv('livedoor_news_corpus.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\nCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
    "print(\"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°:\", len(df))\n",
    "print(\"æœ€åˆã®5ä»¶:\\n\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f7665-cd76-46d6-86ab-f8bd1983d6ff",
   "metadata": {},
   "source": [
    "#### æ—¥æœ¬èªã®åˆ†ã‹ã¡æ›¸ããƒ¡ã‚½ãƒƒãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77634f5-e79d-44db-b2c3-cfbb0238ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from janome.tokenizer import Tokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ===================================================================\n",
    "# ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©é–¢æ•°ï¼ˆã“ã“ã‹ã‚‰ï¼‰\n",
    "# ===================================================================\n",
    "wakati = Tokenizer()\n",
    "\n",
    "def tokenize_ja(sentences_list):\n",
    "    \"\"\"æ—¥æœ¬èªæ–‡ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\"\"\"\n",
    "    wakati_list = []\n",
    "    print(\"ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "    for sentence in tqdm(sentences_list):\n",
    "        wakati_list.append([item.surface for item in wakati.tokenize(sentence)])\n",
    "    return wakati_list\n",
    "\n",
    "def create_word_id_dict(sentences):\n",
    "    \"\"\"å˜èªã‹ã‚‰IDã¸ã®è¾æ›¸ã‚’ç”Ÿæˆ\"\"\"\n",
    "    word_to_id, id_to_word = {}, {}\n",
    "    # 0ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°/æœªçŸ¥èªç”¨ã«äºˆç´„\n",
    "    word_to_id['<PAD>/<UNK>'] = 0\n",
    "    id_to_word[0] = '<PAD>/<UNK>'\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word_to_id:\n",
    "                tmp_id = len(word_to_id)\n",
    "                word_to_id[word] = tmp_id\n",
    "                id_to_word[tmp_id] = word\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def convert_sentences_to_ids(sentences, word_to_id):\n",
    "    \"\"\"æ–‡ç« ã‚’IDåˆ—ã«å¤‰æ›\"\"\"\n",
    "    sentence_id = []\n",
    "    for sentence in sentences:\n",
    "        sentence_ids = [word_to_id.get(word, 0) for word in sentence] # .getã§é«˜é€ŸåŒ–\n",
    "        sentence_id.append(sentence_ids)\n",
    "    return sentence_id\n",
    "\n",
    "def padding_sentence(sentences):\n",
    "    \"\"\"æ–‡ç« ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†\"\"\"\n",
    "    max_len = max(len(s) for s in sentences) if sentences else 0\n",
    "    \n",
    "    padded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        padding = [0] * (max_len - len(sentence))\n",
    "        padded_sentences.append(padding + sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "# ===================================================================\n",
    "# ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©é–¢æ•°ï¼ˆã“ã“ã¾ã§ï¼‰\n",
    "# ===================================================================\n",
    "\n",
    "# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---\n",
    "print(\"1. ç”Ÿãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿...\")\n",
    "df = pd.read_csv('livedoor_news_corpus.csv') # äº‹å‰ã«ä½œæˆã—ãŸCSV\n",
    "\n",
    "# --- ãƒ©ãƒ™ãƒ«ã®IDåŒ– ---\n",
    "print(\"2. ãƒ©ãƒ™ãƒ«ã®IDåŒ–...\")\n",
    "label_to_id = {label: i for i, label in enumerate(df['label'].unique())}\n",
    "id_to_label = {i: label for i, label in enumerate(df['label'].unique())}\n",
    "df['label_id'] = df['label'].map(label_to_id)\n",
    "\n",
    "# --- ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç† ---\n",
    "# 3. ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆåˆ†ã‹ã¡æ›¸ãï¼‰\n",
    "ja_sentences = tokenize_ja(df['text'].tolist())\n",
    "\n",
    "# 4. å˜èªè¾æ›¸ã®ä½œæˆ\n",
    "print(\"4. å˜èªè¾æ›¸ã®ä½œæˆ...\")\n",
    "word_to_id, id_to_word = create_word_id_dict(ja_sentences)\n",
    "\n",
    "# 5. æ–‡ç« ã‚’IDåˆ—ã«å¤‰æ›\n",
    "print(\"5. æ–‡ç« ã‚’IDåˆ—ã«å¤‰æ›...\")\n",
    "sentence_ids = convert_sentences_to_ids(ja_sentences, word_to_id)\n",
    "\n",
    "# 6. ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†\n",
    "print(\"6. ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†...\")\n",
    "padded_ids = padding_sentence(sentence_ids)\n",
    "\n",
    "# --- ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ ---\n",
    "print(\"7. å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜...\")\n",
    "processed_data = {\n",
    "    'padded_ids': padded_ids,\n",
    "    'labels': df['label_id'].tolist(),\n",
    "    'word_to_id': word_to_id,\n",
    "    'id_to_word': id_to_word,\n",
    "    'label_to_id': label_to_id,\n",
    "    'id_to_label': id_to_label,\n",
    "}\n",
    "\n",
    "with open('processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "\n",
    "print(\"\\nğŸ‰ å‰å‡¦ç†ã¨ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
    "print(f\"ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«: processed_data.pkl\")\n",
    "print(f\"èªå½™æ•°: {len(word_to_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd135eaa-d60c-4bae-be8f-db541807a381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00905d48-601b-4f49-9986-0272bb771e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "wakati = Tokenizer()\n",
    "\n",
    "\n",
    "def tokenize_ja(sentences_list):\n",
    "    \"\"\"æ—¥æœ¬èªæ–‡ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "    ä¸ãˆã‚‰ã‚ŒãŸæ—¥æœ¬èªã®æ–‡ã®ãƒªã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã™ã€‚å„æ–‡ã¯å˜èªã«åˆ†å‰²ã•ã‚Œã€ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "    Args:\n",
    "        sentences_list (list): ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹æ—¥æœ¬èªã®æ–‡ã®ãƒªã‚¹ãƒˆã€‚\n",
    "\n",
    "    Returns:\n",
    "        list: ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸæ–‡ã®ãƒªã‚¹ãƒˆã€‚å„è¦ç´ ã¯å˜èªã®ãƒªã‚¹ãƒˆã€‚\n",
    "\n",
    "    Examples:\n",
    "        >>> sentences = [\"ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ\", \"ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆæ–‡ã§ã™ã€‚\"]\n",
    "        >>> tokenize_ja(sentences)\n",
    "        [['ã“ã‚“ã«ã¡ã¯', 'ä¸–ç•Œ'], ['ã“ã‚Œ', 'ã¯', 'ãƒ†ã‚¹ãƒˆ', 'æ–‡', 'ã§ã™', 'ã€‚']]\n",
    "\n",
    "    Note:\n",
    "        - ã“ã®é–¢æ•°ã¯janomeãƒ©ã‚¤ãƒ–ãƒ©ãƒªã® `Tokenizer` ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚\n",
    "        - ä¸ãˆã‚‰ã‚ŒãŸå„æ–‡ã«å¯¾ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å‡¦ç†ã‚’è¡Œã„ã€çµæœã‚’ãƒªã‚¹ãƒˆã§è¿”ã—ã¾ã™ã€‚\n",
    "        - æ—¥æœ¬èªã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã§ã¯ã€å½¢æ…‹ç´ è§£æã‚’è¡Œã„ã€å„å½¢æ…‹ç´ ã®è¡¨å±¤å½¢ã‚’æŠ½å‡ºã—ã¾ã™ã€‚\n",
    "    \"\"\"\n",
    "    wakati_list = []\n",
    "\n",
    "    for sentence in sentences_list:\n",
    "        wakati_list.append([item.surface for item in wakati.tokenize(sentence)])\n",
    "    return wakati_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a94d1a7-bf61-44b1-a204-f05f6b35952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_sentences = tokenize_ja([\"ä¸ãˆã‚‰ã‚ŒãŸæ—¥æœ¬èªã®æ–‡ã®ãƒªã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã™ã€‚\",\"å„æ–‡ã¯å˜èªã«åˆ†å‰²ã•ã‚Œã€ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã•ã‚Œã¾ã™ã€‚\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b83731b4-4389-4de4-b385-2d91e307f31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ä¸ãˆ', 'ã‚‰ã‚Œ', 'ãŸ', 'æ—¥æœ¬èª', 'ã®', 'æ–‡', 'ã®', 'ãƒªã‚¹ãƒˆ', 'ã‚’', 'ãƒˆãƒ¼', 'ã‚¯ãƒ³', 'åŒ–', 'ã—', 'ã¾ã™', 'ã€‚'], ['å„', 'æ–‡', 'ã¯', 'å˜èª', 'ã«', 'åˆ†å‰²', 'ã•', 'ã‚Œ', 'ã€', 'ãƒªã‚¹ãƒˆ', 'ã¨ã—ã¦', 'è¿”ã•', 'ã‚Œ', 'ã¾ã™', 'ã€‚']]\n"
     ]
    }
   ],
   "source": [
    "print(ja_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3f4da-d807-42bf-aef8-165b98851d0a",
   "metadata": {},
   "source": [
    "#### å˜èªã‹ã‚‰IDã¸ã®è¾æ›¸ã‚’ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d3d5f2b-5c00-4976-a3c8-d9f7d897de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_id_dict(sentences):\n",
    "    \"\"\"å˜èªã‹ã‚‰IDã¸ã®è¾æ›¸ã‚’ç”Ÿæˆ\n",
    "    ä¸ãˆã‚‰ã‚ŒãŸæ–‡ã®ãƒªã‚¹ãƒˆã‹ã‚‰ã€å˜èªã‚’IDã«å¤‰æ›ã™ã‚‹ãŸã‚ã®è¾æ›¸ã‚’ç”Ÿæˆã—ã¾ã™ã€‚\n",
    "\n",
    "    Args:\n",
    "        sentences (list): å˜èªã®ãƒªã‚¹ãƒˆã‚’å«ã‚€æ–‡ã®ãƒªã‚¹ãƒˆã€‚\n",
    "\n",
    "    Returns:\n",
    "        dict: å˜èªã‹ã‚‰IDã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å«ã‚€è¾æ›¸ã€‚(word_to_id)\n",
    "                 IDã‹ã‚‰å˜èªã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å«ã‚€è¾æ›¸ã€‚(id_to_word)\n",
    "\n",
    "    Note:\n",
    "        - å„å˜èªã«ä¸€æ„ã®IDã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚\n",
    "        - æœªç™»éŒ²ã®å˜èªãŒã‚ã‚Œã°æ–°ã—ã„IDã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚\n",
    "    \"\"\"\n",
    "    word_to_id, id_to_word = {}, {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word_to_id:\n",
    "                tmp_id = len(word_to_id) + 1\n",
    "                word_to_id[word] = tmp_id\n",
    "                id_to_word[tmp_id] = word\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315fd19-ecae-4ad4-986e-2731cca61155",
   "metadata": {},
   "source": [
    "#### æ–‡ç« ã‚’IDåˆ—ã«å¤‰æ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ce92b66-fbb5-46b8-99ac-af1d5da150cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentences_to_ids(sentences, word_to_id):\n",
    "    \"\"\"æ–‡ç« ã‚’IDåˆ—ã«å¤‰æ›\n",
    "    ä¸ãˆã‚‰ã‚ŒãŸæ–‡ã‚’ã€å˜èªIDã®ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚\n",
    "\n",
    "    Args:\n",
    "        sentences (list): å˜èªã®ãƒªã‚¹ãƒˆã‚’å«ã‚€æ–‡ã®ãƒªã‚¹ãƒˆã€‚\n",
    "        word_to_id (dict): å˜èªã‹ã‚‰IDã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å«ã‚€è¾æ›¸ã€‚\n",
    "\n",
    "    Returns:\n",
    "        list: å˜èªIDã®ãƒªã‚¹ãƒˆã‚’å«ã‚€æ–‡ã®ãƒªã‚¹ãƒˆã€‚\n",
    "\n",
    "    Note:\n",
    "        - è¾æ›¸ã«ç™»éŒ²ã•ã‚Œã¦ã„ãªã„å˜èªã¯ID 0ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã™ã€‚\n",
    "    \"\"\"\n",
    "    sentence_id = []\n",
    "    for sentence in sentences:\n",
    "        sentence_ids = []\n",
    "        for word in sentence:\n",
    "            if word in word_to_id:\n",
    "                sentence_ids.append(word_to_id[word])\n",
    "            else:\n",
    "                sentence_ids.append(0)\n",
    "        sentence_id.append(sentence_ids)\n",
    "    return sentence_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9acd2c7-ceea-4327-ad64-34d7ad908806",
   "metadata": {},
   "source": [
    "#### æ–‡ç« ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c6e59c5-ca61-4817-bc6b-f853281c6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_sentence(sentences):\n",
    "    \"\"\"æ–‡ç« ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†\n",
    "    ä¸ãˆã‚‰ã‚ŒãŸæ–‡ã®ãƒªã‚¹ãƒˆã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ–½ã—ã€å…¨ã¦ã®æ–‡ã®é•·ã•ã‚’çµ±ä¸€ã—ã¾ã™ã€‚\n",
    "\n",
    "    Args:\n",
    "        sentences (list): å˜èªIDã®ãƒªã‚¹ãƒˆã‚’å«ã‚€æ–‡ã®ãƒªã‚¹ãƒˆã€‚\n",
    "\n",
    "    Returns:\n",
    "        list: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸæ–‡ã®ãƒªã‚¹ãƒˆã€‚\n",
    "\n",
    "    Note:\n",
    "        - æœ€ã‚‚é•·ã„æ–‡ã®é•·ã•ã«åˆã‚ã›ã¦ä»–ã®æ–‡ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¾ã™ã€‚\n",
    "        - ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã«ã¯ID 0ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "    \"\"\"\n",
    "    max_sentence_size = 0\n",
    "    for sentence in sentences:\n",
    "        if max_sentence_size < len(sentence):\n",
    "            max_sentence_size = len(sentence)\n",
    "    for sentence in sentences:\n",
    "        while len(sentence) < max_sentence_size:\n",
    "            sentence.insert(0, 0)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51e804fd-333e-45b3-bd85-add7fc3f0661",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m ja_sentences \u001b[38;5;241m=\u001b[39m raw_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# test for 0:99\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m ja_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_ja\u001b[49m\u001b[43m(\u001b[49m\u001b[43mja_sentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m ja_word_to_id, ja_id_to_word \u001b[38;5;241m=\u001b[39m create_word_id_dict(ja_sentences)\n\u001b[0;32m      6\u001b[0m ja_sentences \u001b[38;5;241m=\u001b[39m convert_sentences_to_ids(ja_sentences, ja_word_to_id)\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36mtokenize_ja\u001b[1;34m(sentences_list)\u001b[0m\n\u001b[0;32m     24\u001b[0m wakati_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences_list:\n\u001b[1;32m---> 27\u001b[0m     wakati_list\u001b[38;5;241m.\u001b[39mappend([item\u001b[38;5;241m.\u001b[39msurface \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m wakati\u001b[38;5;241m.\u001b[39mtokenize(sentence)])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wakati_list\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     24\u001b[0m wakati_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences_list:\n\u001b[1;32m---> 27\u001b[0m     wakati_list\u001b[38;5;241m.\u001b[39mappend([item\u001b[38;5;241m.\u001b[39msurface \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m wakati\u001b[38;5;241m.\u001b[39mtokenize(sentence)])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wakati_list\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\tokenizer.py:226\u001b[0m, in \u001b[0;36mTokenizer.__tokenize_stream\u001b[1;34m(self, text, wakati, baseform_unk, dotfile)\u001b[0m\n\u001b[0;32m    224\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m processed \u001b[38;5;241m<\u001b[39m text_length:\n\u001b[1;32m--> 226\u001b[0m     tokens, pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__tokenize_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwakati\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseform_unk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdotfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m token\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\tokenizer.py:250\u001b[0m, in \u001b[0;36mTokenizer.__tokenize_partial\u001b[1;34m(self, text, wakati, baseform_unk, dotfile)\u001b[0m\n\u001b[0;32m    248\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msys_dic\u001b[38;5;241m.\u001b[39mlookup(encoded_partial_text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatcher)\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m entries:\n\u001b[1;32m--> 250\u001b[0m     \u001b[43mlattice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSurfaceNode\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNodeType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSYS_DICT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m matched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(entries) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# unknown\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\lattice.py:139\u001b[0m, in \u001b[0;36mLattice.add\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m    137\u001b[0m dic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdic\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m enode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menodes[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp]:\n\u001b[1;32m--> 139\u001b[0m     cost \u001b[38;5;241m=\u001b[39m enode\u001b[38;5;241m.\u001b[39mmin_cost \u001b[38;5;241m+\u001b[39m \u001b[43mdic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trans_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43menode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_left_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cost \u001b[38;5;241m<\u001b[39m min_cost:\n\u001b[0;32m    141\u001b[0m         min_cost, best_node \u001b[38;5;241m=\u001b[39m cost, enode\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\dic.py:319\u001b[0m, in \u001b[0;36mMMapDictionary.get_trans_cost\u001b[1;34m(self, id1, id2)\u001b[0m\n\u001b[0;32m    316\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mformat_exc()\n\u001b[0;32m    317\u001b[0m         sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_trans_cost\u001b[39m(\u001b[38;5;28mself\u001b[39m, id1, id2):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnections[id1][id2]\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__del__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test for 0:99\n",
    "raw_df = pd.read_csv('livedoor_news_corpus.csv')[0:99]\n",
    "ja_sentences = raw_df[\"text\"].values\n",
    "ja_sentences = tokenize_ja(ja_sentences)\n",
    "ja_word_to_id, ja_id_to_word = create_word_id_dict(ja_sentences)\n",
    "ja_sentences = convert_sentences_to_ids(ja_sentences, ja_word_to_id)\n",
    "ja_sentences = padding_sentence(ja_sentences)\n",
    "ja_sentences = np.array(ja_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8cb60-622c-4f81-9541-ca5a86bda9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_sentences[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fbeead-2aeb-47b1-a94b-7756f9fefa7c",
   "metadata": {},
   "source": [
    "#### å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆStratified Samplingï¼‰\n",
    "ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ã‚«ãƒ†ã‚´ãƒªã‚‚ã€è¨“ç·´ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆã®å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å‡ç­‰ã«åˆ†é…ã•ã‚Œã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’åã‚Šãªãæ­£ç¢ºã«è©•ä¾¡ã§ãã¾ã™ã€‚\n",
    "\n",
    "ã‚‚ã—å˜ç´”ãªãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†ã¨ã€å¶ç„¶ã€ç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ãŒãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã»ã¨ã‚“ã©å«ã¾ã‚Œãªã„ã€ã¨ã„ã£ãŸåã‚ŠãŒç”Ÿã˜ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "stratify=yã‚’æŒ‡å®šã™ã‚‹ã ã‘ã§ã€ãã†ã—ãŸäº‹æ•…ã‚’é˜²ãã€ä¿¡é ¼æ€§ã®é«˜ã„ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "500c84a8-82fe-4e87-b35c-0b5d8d773245",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m ja_sentences\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# ç‰¹å¾´é‡Xï¼ˆæ–‡ç« ï¼‰ã¨ãƒ©ãƒ™ãƒ«yï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‰ã‚’å®šç¾©\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 1. ç¬¬1æ®µéšï¼šè¨“ç·´ï¼‹æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿(90%)ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿(10%)ã«åˆ†å‰²\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# stratify=y ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€yã®æ¯”ç‡ã‚’ä¿ã£ãŸã¾ã¾åˆ†å‰²ã•ã‚Œã‚‹\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# å‰å‡¦ç†æ¸ˆã¿ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "df = ja_sentences\n",
    "\n",
    "#df = ja_sentences\n",
    "# ç‰¹å¾´é‡Xï¼ˆæ–‡ç« ï¼‰ã¨ãƒ©ãƒ™ãƒ«yï¼ˆã‚«ãƒ†ã‚´ãƒªï¼‰ã‚’å®šç¾©\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. ç¬¬1æ®µéšï¼šè¨“ç·´ï¼‹æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿(90%)ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿(10%)ã«åˆ†å‰²\n",
    "# stratify=y ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€yã®æ¯”ç‡ã‚’ä¿ã£ãŸã¾ã¾åˆ†å‰²ã•ã‚Œã‚‹\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.1,      # å…¨ä½“ã®10%ã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«\n",
    "    random_state=42,    # å†ç¾æ€§ã®ãŸã‚ã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰\n",
    "    stratify=y          # å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. ç¬¬2æ®µéšï¼šè¨“ç·´ï¼‹æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²\n",
    "# å…ƒã®90%ã‹ã‚‰ã€ã•ã‚‰ã«1/9ã‚’æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«ã™ã‚‹ã¨ã€å…¨ä½“æ¯”ç‡ãŒ80%:10%ã«ãªã‚‹\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, \n",
    "    test_size=(1/9),    # 90%ã®ã†ã¡ã®1/9ã€ã¤ã¾ã‚Šå…¨ä½“ã®10%\n",
    "    random_state=42,\n",
    "    stratify=y_train_val # ã“ã¡ã‚‰ã‚‚å±¤åŒ–\n",
    ")\n",
    "# --------------------------------------------------\n",
    "\n",
    "# å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª\n",
    "print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿   : {len(X_train)}ä»¶\")\n",
    "print(f\"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿   : {len(X_val)}ä»¶\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ : {len(X_test)}ä»¶\")\n",
    "\n",
    "print(\"\\n--- å„ã‚»ãƒƒãƒˆã®ã‚«ãƒ†ã‚´ãƒªæ¯”ç‡ ---\")\n",
    "print(\"å…ƒãƒ‡ãƒ¼ã‚¿:\\n\", y.value_counts(normalize=True).sort_index())\n",
    "print(\"\\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿:\\n\", y_train.value_counts(normalize=True).sort_index())\n",
    "print(\"\\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿:\\n\", y_test.value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9424bf71-f684-406c-827a-99131ee4f005",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# GiNZAãªã©ã®Tokenizerã¯æº–å‚™æ¸ˆã¿ã¨ä»®å®š\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 1. ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€åˆ†ã‹ã¡æ›¸ã\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlivedoor_news_corpus.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_ja\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# åˆ†ã‹ã¡æ›¸ãé–¢æ•°\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 2. èªå½™ï¼ˆword_to_idè¾æ›¸ï¼‰ã‚’æ§‹ç¯‰\u001b[39;00m\n\u001b[0;32m     12\u001b[0m word_counter \u001b[38;5;241m=\u001b[39m Counter(word \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36mtokenize_ja\u001b[1;34m(sentences_list)\u001b[0m\n\u001b[0;32m     24\u001b[0m wakati_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences_list:\n\u001b[1;32m---> 27\u001b[0m     wakati_list\u001b[38;5;241m.\u001b[39mappend([item\u001b[38;5;241m.\u001b[39msurface \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m wakati\u001b[38;5;241m.\u001b[39mtokenize(sentence)])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wakati_list\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     24\u001b[0m wakati_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences_list:\n\u001b[1;32m---> 27\u001b[0m     wakati_list\u001b[38;5;241m.\u001b[39mappend([item\u001b[38;5;241m.\u001b[39msurface \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m wakati\u001b[38;5;241m.\u001b[39mtokenize(sentence)])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wakati_list\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\tokenizer.py:226\u001b[0m, in \u001b[0;36mTokenizer.__tokenize_stream\u001b[1;34m(self, text, wakati, baseform_unk, dotfile)\u001b[0m\n\u001b[0;32m    224\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m processed \u001b[38;5;241m<\u001b[39m text_length:\n\u001b[1;32m--> 226\u001b[0m     tokens, pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__tokenize_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwakati\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseform_unk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdotfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m token\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\tokenizer.py:248\u001b[0m, in \u001b[0;36mTokenizer.__tokenize_partial\u001b[1;34m(self, text, wakati, baseform_unk, dotfile)\u001b[0m\n\u001b[0;32m    245\u001b[0m     matched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(entries) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# system dictionary\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msys_dic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_partial_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m entries:\n\u001b[0;32m    250\u001b[0m     lattice\u001b[38;5;241m.\u001b[39madd(SurfaceNode(e, NodeType\u001b[38;5;241m.\u001b[39mSYS_DICT))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\dic.py:253\u001b[0m, in \u001b[0;36mMMapDictionary.lookup\u001b[1;34m(self, s, matcher)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlookup\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, matcher):\n\u001b[1;32m--> 253\u001b[0m     (matched, outputs) \u001b[38;5;241m=\u001b[39m \u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matched:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\fst.py:341\u001b[0m, in \u001b[0;36mMatcher.run\u001b[1;34m(self, word, common_prefix_match)\u001b[0m\n\u001b[0;32m    339\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdict_len):\n\u001b[1;32m--> 341\u001b[0m     output \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_prefix_match\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(output), output\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\avilen-e\\lib\\site-packages\\janome\\fst.py:364\u001b[0m, in \u001b[0;36mMatcher._run\u001b[1;34m(self, word, data_num, common_prefix_match)\u001b[0m\n\u001b[0;32m    361\u001b[0m         i \u001b[38;5;241m=\u001b[39m j\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_len\u001b[49m:\n\u001b[0;32m    365\u001b[0m     arc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_arc(data, pos)\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;66;03m# arc[0]: flag\u001b[39;00m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;66;03m# arc[1]: label\u001b[39;00m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;66;03m# arc[2]: output\u001b[39;00m\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;66;03m# arc[3]: final_output\u001b[39;00m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;66;03m# arc[4]: target\u001b[39;00m\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;66;03m# arc[5]: incr\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "# GiNZAãªã©ã®Tokenizerã¯æº–å‚™æ¸ˆã¿ã¨ä»®å®š\n",
    "\n",
    "\n",
    "# 1. ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€åˆ†ã‹ã¡æ›¸ã\n",
    "df = pd.read_csv('livedoor_news_corpus.csv')\n",
    "df['tokens'] = df['text'].apply(tokenize_ja) # åˆ†ã‹ã¡æ›¸ãé–¢æ•°\n",
    "\n",
    "# 2. èªå½™ï¼ˆword_to_idè¾æ›¸ï¼‰ã‚’æ§‹ç¯‰\n",
    "word_counter = Counter(word for tokens in df['tokens'] for word in tokens)\n",
    "word_to_id = {word: i+2 for i, (word, count) in enumerate(word_counter.items())} # 0ã¨1ã¯ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ç”¨\n",
    "word_to_id['<PAD>'] = 0\n",
    "word_to_id['<UNK>'] = 1\n",
    "\n",
    "# 3. ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ©ãƒ™ãƒ«ã‚’IDã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¤‰æ›\n",
    "df['input_ids'] = df['tokens'].apply(lambda tokens: [word_to_id.get(w, word_to_id['<UNK>']) for w in tokens])\n",
    "# ãƒ©ãƒ™ãƒ«ã®IDåŒ–...\n",
    "\n",
    "# 4. å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
    "processed_data = {\n",
    "    'input_ids': df['input_ids'].tolist(),\n",
    "    'labels': df['label_id'].tolist(),\n",
    "    'word_to_id': word_to_id,\n",
    "    # ä»–ã«ã‚‚å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°...\n",
    "}\n",
    "\n",
    "with open('processed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_data, f)\n",
    "\n",
    "print(\"å‰å‡¦ç†ã¨ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d84475-4430-45b6-b792-35bf34505d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
